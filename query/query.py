"""
Script de consulta implementando RAG (Retrieval-Augmented Generation).

Este m√≥dulo permite fazer perguntas sobre as normas da UFCSPA usando
busca vetorial para recuperar contexto relevante dos documentos.
"""

import argparse
import json
import logging
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer


# Configura√ß√£o do logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


class QueryEngine:
    """Motor de consulta RAG para buscar informa√ß√µes nas normas da UFCSPA.
    
    Esta classe implementa o pipeline completo de RAG (Retrieval-Augmented Generation),
    desde a busca vetorial at√© a constru√ß√£o do contexto para gera√ß√£o de resposta.
    
    Attributes:
        index_dir: Diret√≥rio contendo o √≠ndice FAISS e metadados
        model: Modelo sentence-transformers para gerar embeddings
        index: √çndice FAISS carregado
        chunks: Lista de chunks de texto com metadados
        k: N√∫mero de chunks a recuperar para cada consulta
    """
    
    def __init__(
        self,
        index_dir: str = "faiss_index",
        k: int = 5
    ):
        """Inicializa o motor de consulta.
        
        Args:
            index_dir: Diret√≥rio contendo o √≠ndice FAISS
            k: N√∫mero de chunks a recuperar (top-k)
        """
        self.index_dir = Path(index_dir)
        self.k = k
        
        logger.info("Inicializando QueryEngine...")
        
        # Carrega informa√ß√µes do √≠ndice
        self._load_index_info()
        
        # Carrega o modelo de embeddings
        logger.info(f"Carregando modelo: {self.model_name}")
        self.model = SentenceTransformer(self.model_name)
        self.model.to('cpu')
        
        # Carrega o √≠ndice FAISS
        self._load_faiss_index()
        
        # Carrega os chunks
        self._load_chunks()
        
        logger.info(f"QueryEngine pronto! ({self.n_chunks} chunks dispon√≠veis)")
    
    def _load_index_info(self):
        """Carrega as informa√ß√µes do √≠ndice."""
        info_file = self.index_dir / "index_info.json"
        
        if not info_file.exists():
            raise FileNotFoundError(f"Arquivo de informa√ß√µes n√£o encontrado: {info_file}")
        
        with open(info_file, 'r') as f:
            info = json.load(f)
        
        self.model_name = info['model_name']
        self.embedding_dim = info['embedding_dim']
        self.n_vectors = info['n_vectors']
        
        logger.info(f"Informa√ß√µes do √≠ndice carregadas: {self.n_vectors} vetores, dimens√£o {self.embedding_dim}")
    
    def _load_faiss_index(self):
        """Carrega o √≠ndice FAISS."""
        index_file = self.index_dir / "ufcspa.index"
        
        if not index_file.exists():
            raise FileNotFoundError(f"√çndice FAISS n√£o encontrado: {index_file}")
        
        self.index = faiss.read_index(str(index_file))
        
        if self.index.ntotal != self.n_vectors:
            logger.warning(f"N√∫mero de vetores no √≠ndice ({self.index.ntotal}) difere do esperado ({self.n_vectors})")
        
        logger.info(f"√çndice FAISS carregado: {self.index.ntotal} vetores")
    
    def _load_chunks(self):
        """Carrega os chunks de texto."""
        chunks_file = self.index_dir / "chunks.json"
        
        if not chunks_file.exists():
            raise FileNotFoundError(f"Arquivo de chunks n√£o encontrado: {chunks_file}")
        
        with open(chunks_file, 'r', encoding='utf-8') as f:
            self.chunks = json.load(f)
        
        self.n_chunks = len(self.chunks)
        logger.info(f"Carregados {self.n_chunks} chunks")
    
    def query(self, question: str, k: Optional[int] = None, use_llm: bool = True) -> Dict:
        """Executa uma consulta RAG completa.
        
        Args:
            question: Pergunta do usu√°rio
            k: N√∫mero de chunks a recuperar (usa self.k se n√£o especificado)
            use_llm: Se True, usa Llama para gerar resposta
            
        Returns:
            Dict contendo os resultados da busca e a resposta gerada
        """
        if not question or not question.strip():
            raise ValueError("Pergunta n√£o pode estar vazia")
        
        k = k or self.k
        logger.info(f"Processando consulta: '{question}' (k={k})")
        
        # Gera embedding da pergunta
        query_embedding = self._generate_query_embedding(question)
        
        # Busca chunks relevantes
        results = self._search_similar_chunks(query_embedding, k)
        
        # Constr√≥i o contexto
        context = self._build_context(results)
        
        # Gera resposta com Llama se solicitado
        if use_llm:
            try:
                from .llama_model import get_llama_model
                llama = get_llama_model()
                
                # Verifica se o modelo est√° dispon√≠vel
                model_info = llama.get_model_info()
                if model_info['status'] == 'ready':
                    prompt = llama.format_prompt(context, question)
                    answer = llama.generate(prompt)
                else:
                    answer = self._build_prompt(question, context, results)
                    answer += f"\n\n[Status do Llama: {model_info['status']} - {model_info.get('message', '')}]"
            except ImportError:
                answer = self._build_prompt(question, context, results)
                answer += "\n\n[Llama n√£o dispon√≠vel - Instale llama-cpp-python]"
        else:
            answer = self._build_prompt(question, context, results)
        
        return {
            'question': question,
            'results': results,
            'context': context,
            'answer': answer,
            'k': k
        }
    
    def _generate_query_embedding(self, question: str) -> np.ndarray:
        """Gera embedding para a pergunta do usu√°rio.
        
        Args:
            question: Pergunta do usu√°rio
            
        Returns:
            Array numpy com o embedding normalizado
        """
        # Gera embedding
        embedding = self.model.encode(
            [question],
            convert_to_numpy=True,
            show_progress_bar=False
        )
        
        # Normaliza para usar com IndexFlatIP
        embedding = embedding / np.linalg.norm(embedding)
        
        return embedding.astype('float32')
    
    def _search_similar_chunks(self, query_embedding: np.ndarray, k: int) -> List[Dict]:
        """Busca os k chunks mais similares √† consulta.
        
        Args:
            query_embedding: Embedding da consulta
            k: N√∫mero de resultados a retornar
            
        Returns:
            Lista de dicion√°rios com os chunks e seus scores
        """
        # Garante que k n√£o exceda o n√∫mero de chunks
        k = min(k, self.n_chunks)
        
        # Busca no √≠ndice FAISS
        scores, indices = self.index.search(query_embedding, k)
        
        # Prepara os resultados
        results = []
        for idx, score in zip(indices[0], scores[0]):
            if idx < 0 or idx >= self.n_chunks:
                logger.warning(f"√çndice inv√°lido retornado: {idx}")
                continue
            
            chunk = self.chunks[idx]
            result = {
                'chunk_id': chunk['chunk_id'],
                'source_file': chunk['source_file'],
                'text': chunk['text'],
                'score': float(score),
                'index': int(idx)
            }
            results.append(result)
        
        logger.info(f"Encontrados {len(results)} chunks relevantes")
        return results
    
    def _build_context(self, results: List[Dict]) -> str:
        """Constr√≥i o contexto concatenando os textos dos chunks.
        
        Args:
            results: Lista de resultados da busca
            
        Returns:
            String com o contexto concatenado
        """
        context_parts = []
        
        for i, result in enumerate(results):
            source = result['source_file']
            chunk_id = result['chunk_id']
            text = result['text']
            score = result['score']
            
            # Adiciona cabe√ßalho com informa√ß√µes do chunk
            header = f"[Documento: {source} | Chunk: {chunk_id} | Relev√¢ncia: {score:.3f}]"
            context_parts.append(header)
            context_parts.append(text)
            
            # Adiciona separador entre chunks (exceto no √∫ltimo)
            if i < len(results) - 1:
                context_parts.append("---")
        
        return '\n'.join(context_parts)
    
    def _build_prompt(self, question: str, context: str, results: List[Dict]) -> str:
        """Constr√≥i o prompt final para o LLM.
        
        Args:
            question: Pergunta original do usu√°rio
            context: Contexto constru√≠do dos chunks
            results: Lista de resultados para estat√≠sticas
            
        Returns:
            String com o prompt formatado
        """
        # Template do prompt
        prompt = f"""### SISTEMA DE CONSULTA - NORMAS UFCSPA ###

Voc√™ √© um assistente especializado em normas e regulamentos da UFCSPA (Universidade Federal de Ci√™ncias da Sa√∫de de Porto Alegre). Use o contexto dos documentos abaixo para responder √† pergunta do usu√°rio de forma precisa e completa.

### CONTEXTO DOS DOCUMENTOS ###
{context}

### PERGUNTA DO USU√ÅRIO ###
{question}

### INSTRU√á√ïES ###
1. Baseie sua resposta APENAS nas informa√ß√µes presentes no contexto acima
2. Se a informa√ß√£o n√£o estiver no contexto, informe claramente
3. Cite os documentos relevantes quando poss√≠vel
4. Seja objetivo e direto na resposta

### Resposta Gerada por IA (Placeholder) ###
[NOTA: Em um sistema completo, aqui seria gerada uma resposta usando um LLM como GPT-4, Llama, ou similar]

Com base no contexto acima dos {len(results)} chunks mais relevantes encontrados, a resposta seria gerada aqui.

### METADADOS DA BUSCA ###
- Pergunta: "{question}"
- Chunks recuperados: {len(results)}
- Documentos √∫nicos: {len(set(r['source_file'] for r in results))}
- Score m√©dio de relev√¢ncia: {np.mean([r['score'] for r in results]):.3f}
"""
        
        return prompt


def format_results(query_result: Dict, verbose: bool = False):
    """Formata e imprime os resultados da consulta.
    
    Args:
        query_result: Dicion√°rio com os resultados da consulta
        verbose: Se True, mostra informa√ß√µes detalhadas
    """
    print("\n" + "=" * 80)
    print("RESULTADOS DA CONSULTA RAG - UFCSPA")
    print("=" * 80)
    
    print(f"\nüìù PERGUNTA: {query_result['question']}")
    
    print(f"\nüîç BUSCA VETORIAL:")
    print(f"   - Chunks recuperados: {len(query_result['results'])}")
    
    if verbose:
        print("\nüìÑ CHUNKS ENCONTRADOS:")
        for i, result in enumerate(query_result['results'], 1):
            print(f"\n   [{i}] Documento: {result['source_file']}")
            print(f"       Chunk ID: {result['chunk_id']}")
            print(f"       Relev√¢ncia: {result['score']:.3f}")
            print(f"       Preview: {result['text'][:150]}...")
    
    print("\n" + "-" * 80)
    print("üí¨ RESPOSTA GERADA:")
    print("-" * 80)
    
    # Mostra a resposta gerada ou o prompt
    if 'answer' in query_result:
        print(query_result['answer'])
    else:
        print(query_result.get('prompt', 'Nenhuma resposta gerada'))
    
    print("=" * 80)


def main():
    """Fun√ß√£o principal do script de consulta."""
    parser = argparse.ArgumentParser(
        description="Sistema de consulta RAG para normas da UFCSPA",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  python query.py "Qual o regimento para atividades de extens√£o?"
  python query.py "Como funciona o processo de matr√≠cula?" -k 3
  python query.py "Normas para afastamento docente" -v
        """
    )
    
    parser.add_argument(
        'question',
        type=str,
        help='Pergunta sobre as normas da UFCSPA'
    )
    
    parser.add_argument(
        '-k', '--top-k',
        type=int,
        default=5,
        help='N√∫mero de chunks a recuperar (padr√£o: 5)'
    )
    
    parser.add_argument(
        '-v', '--verbose',
        action='store_true',
        help='Mostra informa√ß√µes detalhadas sobre os chunks encontrados'
    )
    
    parser.add_argument(
        '--index-dir',
        type=str,
        default='faiss_index',
        help='Diret√≥rio contendo o √≠ndice FAISS (padr√£o: faiss_index)'
    )
    
    args = parser.parse_args()
    
    try:
        # Inicializa o motor de consulta
        engine = QueryEngine(
            index_dir=args.index_dir,
            k=args.top_k
        )
        
        # Executa a consulta
        result = engine.query(args.question, k=args.top_k)
        
        # Formata e exibe os resultados
        format_results(result, verbose=args.verbose)
        
    except FileNotFoundError as e:
        logger.error(f"Erro: {e}")
        logger.error("Execute primeiro o pipeline de ingest√£o para criar o √≠ndice FAISS")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Erro ao processar consulta: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()