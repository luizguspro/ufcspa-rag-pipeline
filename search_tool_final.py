"""
search_tool_final.py - Vers√£o Refatorada
Sistema de busca vetorial para documentos da UFCSPA com pipeline otimizada

Esta vers√£o implementa:
1. Carregamento inteligente de documentos
2. Deduplica√ß√£o de conte√∫do
3. Chunking sem√¢ntico que preserva contexto
4. Busca vetorial eficiente com FAISS
5. Prepara√ß√£o para re-ranking futuro
"""

import os
import hashlib
import logging
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import pickle

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.schema import Document
from tqdm import tqdm

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class VectorSearchTool:
    """
    Ferramenta de busca vetorial otimizada para documentos da UFCSPA.
    
    Esta classe gerencia todo o pipeline de prepara√ß√£o de dados e busca,
    incluindo carregamento, deduplica√ß√£o, chunking inteligente e busca vetorial.
    """
    
    def __init__(
        self,
        documents_dir: str = "data/processed",
        vectorstore_dir: str = "vectorstore",
        embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2",
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        force_rebuild: bool = False
    ):
        """
        Inicializa a ferramenta de busca vetorial.
        
        Args:
            documents_dir: Diret√≥rio contendo os documentos de texto
            vectorstore_dir: Diret√≥rio para armazenar o banco vetorial
            embedding_model: Modelo de embeddings a usar
            chunk_size: Tamanho m√°ximo de cada chunk em caracteres
            chunk_overlap: Sobreposi√ß√£o entre chunks consecutivos
            force_rebuild: Se True, reconstr√≥i o banco vetorial mesmo se j√° existir
        """
        self.documents_dir = Path(documents_dir)
        self.vectorstore_dir = Path(vectorstore_dir)
        self.embedding_model_name = embedding_model
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # Cria diret√≥rios se n√£o existirem
        self.vectorstore_dir.mkdir(parents=True, exist_ok=True)
        
        # Inicializa o modelo de embeddings
        logger.info(f"Inicializando modelo de embeddings: {embedding_model}")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # Carrega ou cria o banco vetorial
        self.vectorstore = self._load_or_create_vectorstore(force_rebuild)
        
        logger.info("VectorSearchTool inicializada com sucesso")
    
    def _load_documents(self) -> List[Document]:
        """
        Carrega todos os documentos de texto do diret√≥rio especificado.
        
        Returns:
            Lista de documentos LangChain
        """
        logger.info(f"Carregando documentos de: {self.documents_dir}")
        
        # Usa DirectoryLoader para carregar todos os arquivos .txt
        loader = DirectoryLoader(
            str(self.documents_dir),
            glob="**/*.txt",
            loader_cls=TextLoader,
            loader_kwargs={'encoding': 'utf-8'},
            show_progress=True
        )
        
        try:
            documents = loader.load()
            logger.info(f"Carregados {len(documents)} documentos")
            return documents
        except Exception as e:
            logger.error(f"Erro ao carregar documentos: {e}")
            return []
    
    def _deduplicate_documents(self, documents: List[Document]) -> List[Document]:
        """
        Remove documentos duplicados baseado no hash do conte√∫do.
        
        Args:
            documents: Lista de documentos originais
            
        Returns:
            Lista de documentos √∫nicos
        """
        logger.info("Iniciando deduplica√ß√£o de documentos...")
        
        seen_hashes = set()
        unique_documents = []
        duplicates_count = 0
        
        for doc in documents:
            # Cria hash do conte√∫do do documento
            content_hash = hashlib.md5(doc.page_content.encode()).hexdigest()
            
            if content_hash not in seen_hashes:
                seen_hashes.add(content_hash)
                unique_documents.append(doc)
            else:
                duplicates_count += 1
        
        logger.info(f"Removidos {duplicates_count} documentos duplicados")
        logger.info(f"Documentos √∫nicos restantes: {len(unique_documents)}")
        
        return unique_documents
    
    def _smart_chunk_documents(self, documents: List[Document]) -> List[Document]:
        """
        Divide documentos em chunks usando estrat√©gia inteligente que preserva contexto.
        
        Args:
            documents: Lista de documentos para dividir
            
        Returns:
            Lista de chunks (documentos menores)
        """
        logger.info("Iniciando chunking inteligente de documentos...")
        
        # Configura o splitter com hierarquia de separadores
        # Prioriza quebras por: par√°grafos duplos > par√°grafo √∫nico > linha > ponto > espa√ßo
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=len,
            separators=[
                "\n\n\n",  # M√∫ltiplas quebras (se√ß√µes)
                "\n\n",    # Par√°grafos
                "\n",      # Linhas
                ". ",      # Senten√ßas
                "! ",      # Exclama√ß√µes
                "? ",      # Perguntas
                "; ",      # Ponto e v√≠rgula
                ", ",      # V√≠rgulas
                " ",       # Espa√ßos
                ""         # Caracteres individuais (√∫ltimo recurso)
            ]
        )
        
        # Processa cada documento
        all_chunks = []
        for doc in tqdm(documents, desc="Dividindo documentos em chunks"):
            chunks = text_splitter.split_documents([doc])
            
            # Adiciona metadados √∫teis a cada chunk
            for i, chunk in enumerate(chunks):
                chunk.metadata.update({
                    'chunk_index': i,
                    'total_chunks': len(chunks),
                    'source_file': doc.metadata.get('source', 'unknown')
                })
            
            all_chunks.extend(chunks)
        
        logger.info(f"Criados {len(all_chunks)} chunks a partir de {len(documents)} documentos")
        
        # Remove chunks muito pequenos (provavelmente s√≥ t√™m pontua√ß√£o ou espa√ßos)
        filtered_chunks = [
            chunk for chunk in all_chunks 
            if len(chunk.page_content.strip()) > 50
        ]
        
        if len(filtered_chunks) < len(all_chunks):
            removed = len(all_chunks) - len(filtered_chunks)
            logger.info(f"Removidos {removed} chunks muito pequenos")
        
        return filtered_chunks
    
    def _deduplicate_chunks(self, chunks: List[Document]) -> List[Document]:
        """
        Remove chunks com conte√∫do muito similar para evitar redund√¢ncia.
        
        Args:
            chunks: Lista de chunks originais
            
        Returns:
            Lista de chunks √∫nicos
        """
        logger.info("Deduplicando chunks similares...")
        
        seen_contents = set()
        unique_chunks = []
        duplicates_count = 0
        
        for chunk in chunks:
            # Normaliza o conte√∫do para compara√ß√£o
            # Remove espa√ßos extras e converte para min√∫sculas
            normalized_content = ' '.join(chunk.page_content.lower().split())
            
            # Cria hash do conte√∫do normalizado
            content_hash = hashlib.md5(normalized_content.encode()).hexdigest()
            
            if content_hash not in seen_contents:
                seen_contents.add(content_hash)
                unique_chunks.append(chunk)
            else:
                duplicates_count += 1
        
        logger.info(f"Removidos {duplicates_count} chunks duplicados")
        logger.info(f"Chunks √∫nicos finais: {len(unique_chunks)}")
        
        return unique_chunks
    
    def _create_vectorstore(self, chunks: List[Document]) -> FAISS:
        """
        Cria um banco vetorial FAISS a partir dos chunks.
        
        Args:
            chunks: Lista de chunks para vetorizar
            
        Returns:
            Banco vetorial FAISS
        """
        logger.info("Criando banco vetorial FAISS...")
        
        # Cria o banco vetorial com barra de progresso
        vectorstore = FAISS.from_documents(
            documents=chunks,
            embedding=self.embeddings
        )
        
        logger.info(f"Banco vetorial criado com {len(chunks)} documentos")
        
        return vectorstore
    
    def _save_vectorstore(self, vectorstore: FAISS):
        """
        Salva o banco vetorial no disco para uso futuro.
        
        Args:
            vectorstore: Banco vetorial a salvar
        """
        logger.info(f"Salvando banco vetorial em: {self.vectorstore_dir}")
        
        # Salva o banco vetorial
        vectorstore.save_local(str(self.vectorstore_dir))
        
        # Salva metadados adicionais
        metadata = {
            'embedding_model': self.embedding_model_name,
            'chunk_size': self.chunk_size,
            'chunk_overlap': self.chunk_overlap,
            'total_chunks': len(vectorstore.docstore._dict)
        }
        
        metadata_path = self.vectorstore_dir / "metadata.pkl"
        with open(metadata_path, 'wb') as f:
            pickle.dump(metadata, f)
        
        logger.info("Banco vetorial salvo com sucesso")
    
    def _load_vectorstore(self) -> Optional[FAISS]:
        """
        Carrega um banco vetorial existente do disco.
        
        Returns:
            Banco vetorial FAISS ou None se n√£o existir
        """
        index_path = self.vectorstore_dir / "index.faiss"
        
        if not index_path.exists():
            logger.info("Banco vetorial n√£o encontrado")
            return None
        
        logger.info("Carregando banco vetorial existente...")
        
        try:
            vectorstore = FAISS.load_local(
                str(self.vectorstore_dir),
                self.embeddings
            )
            
            # Carrega metadados
            metadata_path = self.vectorstore_dir / "metadata.pkl"
            if metadata_path.exists():
                with open(metadata_path, 'rb') as f:
                    metadata = pickle.load(f)
                logger.info(f"Banco vetorial carregado: {metadata['total_chunks']} chunks")
            
            return vectorstore
            
        except Exception as e:
            logger.error(f"Erro ao carregar banco vetorial: {e}")
            return None
    
    def _load_or_create_vectorstore(self, force_rebuild: bool = False) -> FAISS:
        """
        Carrega um banco vetorial existente ou cria um novo se necess√°rio.
        
        Args:
            force_rebuild: Se True, sempre reconstr√≥i o banco
            
        Returns:
            Banco vetorial FAISS
        """
        # Tenta carregar banco existente se n√£o for for√ßado rebuild
        if not force_rebuild:
            vectorstore = self._load_vectorstore()
            if vectorstore is not None:
                return vectorstore
        
        # Cria novo banco vetorial
        logger.info("Construindo novo banco vetorial...")
        
        # Pipeline de prepara√ß√£o de dados
        documents = self._load_documents()
        if not documents:
            raise ValueError("Nenhum documento encontrado para processar")
        
        # Deduplica√ß√£o em n√≠vel de documento
        documents = self._deduplicate_documents(documents)
        
        # Chunking inteligente
        chunks = self._smart_chunk_documents(documents)
        
        # Deduplica√ß√£o em n√≠vel de chunk
        chunks = self._deduplicate_chunks(chunks)
        
        # Cria√ß√£o do banco vetorial
        vectorstore = self._create_vectorstore(chunks)
        
        # Salva para uso futuro
        self._save_vectorstore(vectorstore)
        
        return vectorstore
    
    def search(
        self,
        query: str,
        k: int = 5,
        fetch_k: int = 20,
        filter_dict: Optional[Dict] = None
    ) -> List[str]:
        """
        Realiza busca vetorial por similaridade.
        
        Args:
            query: Pergunta ou consulta do usu√°rio
            k: N√∫mero de resultados finais a retornar
            fetch_k: N√∫mero de candidatos iniciais para buscar (para re-ranking futuro)
            filter_dict: Filtros opcionais para aplicar na busca
            
        Returns:
            Lista com o conte√∫do dos k chunks mais relevantes
        """
        logger.info(f"Realizando busca: '{query}'")
        
        try:
            # Busca inicial com mais candidatos para permitir re-ranking futuro
            results = self.vectorstore.similarity_search_with_score(
                query=query,
                k=fetch_k,
                filter=filter_dict
            )
            
            # TODO: Implementar um Cross-Encoder para re-ranking dos resultados aqui
            # O Cross-Encoder receberia a query e cada documento candidato,
            # calcularia um score mais preciso e reordenaria os resultados.
            # Por enquanto, usamos apenas os top-k resultados da busca vetorial.
            
            # Extrai apenas os top-k resultados
            top_results = results[:k]
            
            # Log dos scores para debug
            for i, (doc, score) in enumerate(top_results):
                logger.debug(f"Resultado {i+1} - Score: {score:.4f} - "
                           f"Fonte: {doc.metadata.get('source', 'unknown')}")
            
            # Retorna apenas o conte√∫do dos chunks
            chunks_content = [doc.page_content for doc, _ in top_results]
            
            logger.info(f"Retornando {len(chunks_content)} resultados relevantes")
            
            return chunks_content
            
        except Exception as e:
            logger.error(f"Erro durante a busca: {e}")
            return []
    
    def search_with_metadata(
        self,
        query: str,
        k: int = 5,
        fetch_k: int = 20,
        filter_dict: Optional[Dict] = None
    ) -> List[Tuple[str, Dict, float]]:
        """
        Realiza busca retornando conte√∫do, metadados e scores.
        
        √ötil para debug e an√°lise dos resultados.
        
        Args:
            query: Pergunta ou consulta do usu√°rio
            k: N√∫mero de resultados finais a retornar
            fetch_k: N√∫mero de candidatos iniciais
            filter_dict: Filtros opcionais
            
        Returns:
            Lista de tuplas (conte√∫do, metadados, score)
        """
        logger.info(f"Realizando busca com metadados: '{query}'")
        
        try:
            results = self.vectorstore.similarity_search_with_score(
                query=query,
                k=fetch_k,
                filter=filter_dict
            )
            
            # TODO: Cross-Encoder para re-ranking aqui
            
            top_results = results[:k]
            
            # Prepara resultados com todas as informa√ß√µes
            detailed_results = [
                (doc.page_content, doc.metadata, score)
                for doc, score in top_results
            ]
            
            return detailed_results
            
        except Exception as e:
            logger.error(f"Erro durante a busca: {e}")
            return []


# Inst√¢ncia global para compatibilidade com a interface anterior
_search_tool_instance = None


def get_search_tool(**kwargs) -> VectorSearchTool:
    """
    Retorna uma inst√¢ncia singleton da ferramenta de busca.
    
    Args:
        **kwargs: Argumentos para VectorSearchTool
        
    Returns:
        Inst√¢ncia de VectorSearchTool
    """
    global _search_tool_instance
    if _search_tool_instance is None:
        _search_tool_instance = VectorSearchTool(**kwargs)
    return _search_tool_instance


def search_vectorstore(query: str, k: int = 5) -> List[str]:
    """
    Interface compat√≠vel com a vers√£o anterior do script.
    
    Args:
        query: Pergunta do usu√°rio
        k: N√∫mero de resultados a retornar
        
    Returns:
        Lista com o conte√∫do dos chunks mais relevantes
    """
    tool = get_search_tool()
    return tool.search(query, k=k)


# Bloco de teste e demonstra√ß√£o
if __name__ == '__main__':
    print("=" * 70)
    print("FERRAMENTA DE BUSCA VETORIAL - VERS√ÉO REFATORADA")
    print("=" * 70)
    
    # Pergunta de teste
    pergunta_exemplo = "Quais s√£o as normas para atividades de extens√£o universit√°ria?"
    
    print(f"\nüîç Pergunta: {pergunta_exemplo}")
    print("-" * 70)
    
    try:
        # Inicializa a ferramenta (criar√° o banco vetorial se necess√°rio)
        print("\n‚öôÔ∏è  Inicializando ferramenta de busca...")
        tool = VectorSearchTool(
            documents_dir="data/processed",
            force_rebuild=False  # Mude para True para reconstruir o banco
        )
        
        # Realiza a busca
        print("\nüîé Realizando busca...")
        resultados = tool.search_with_metadata(pergunta_exemplo, k=5)
        
        if not resultados:
            print("\n‚ùå Nenhum resultado encontrado")
        else:
            print(f"\n‚úÖ Encontrados {len(resultados)} resultados relevantes:\n")
            
            for i, (conteudo, metadata, score) in enumerate(resultados, 1):
                print(f"{'='*70}")
                print(f"RESULTADO {i}")
                print(f"Score: {score:.4f}")
                print(f"Fonte: {metadata.get('source', 'unknown')}")
                print(f"Chunk: {metadata.get('chunk_index', '?')} de {metadata.get('total_chunks', '?')}")
                print(f"-" * 70)
                print(f"{conteudo[:500]}..." if len(conteudo) > 500 else conteudo)
                print()
        
        # Demonstra uso simples para integra√ß√£o
        print("\n" + "="*70)
        print("USO SIMPLES PARA INTEGRA√á√ÉO:")
        print("="*70)
        
        contextos = search_vectorstore(pergunta_exemplo)
        print(f"\nContextos prontos para LLM: {len(contextos)} chunks")
        print("\nPrimeiro contexto (preview):")
        print(contextos[0][:200] + "..." if contextos and len(contextos[0]) > 200 else contextos[0] if contextos else "Nenhum")
        
    except Exception as e:
        print(f"\n‚ùå Erro: {e}")
        import traceback
        traceback.print_exc()